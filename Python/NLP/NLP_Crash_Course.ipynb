{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "059Gun7ndVzt",
        "YkI1nZ6ggl05",
        "YWnZo2QdhcNY",
        "fOriYovWvt39",
        "lO84HnlJvvat",
        "_4BwxYicJovp",
        "uTo15BMwKxLe"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6631d06f0c544c8fb66533bef14958a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_934b66da556d41f28d48e86f8fa8c89b",
              "IPY_MODEL_653f6256421b4656a176c0cb97ab5c7c",
              "IPY_MODEL_5022bcdcd7964b50ad34856693587219"
            ],
            "layout": "IPY_MODEL_376e67834033415db5f9c9407f9b369c"
          }
        },
        "934b66da556d41f28d48e86f8fa8c89b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49912788b73849a6944a04be7baa7c7f",
            "placeholder": "​",
            "style": "IPY_MODEL_0e7a79c5e2764d2681913b43161cbc14",
            "value": "Map: 100%"
          }
        },
        "653f6256421b4656a176c0cb97ab5c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d08cc437a4c84a8cb5c513c3523f87ea",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41ca3d16a7a0453ea25f4f6d78f4b717",
            "value": 25000
          }
        },
        "5022bcdcd7964b50ad34856693587219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c6430a7544c408b955b5bab990b4955",
            "placeholder": "​",
            "style": "IPY_MODEL_8255e9e3e456427197606492f5b2f85f",
            "value": " 25000/25000 [02:18&lt;00:00, 193.81 examples/s]"
          }
        },
        "376e67834033415db5f9c9407f9b369c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49912788b73849a6944a04be7baa7c7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e7a79c5e2764d2681913b43161cbc14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d08cc437a4c84a8cb5c513c3523f87ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41ca3d16a7a0453ea25f4f6d78f4b717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c6430a7544c408b955b5bab990b4955": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8255e9e3e456427197606492f5b2f85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3907823f5df48ba8be598fde00d85ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84e8de2ed794435a874c16b5602980a2",
              "IPY_MODEL_c7bdab274bb84dc28e8ab950164bcceb",
              "IPY_MODEL_6648299b768b4842ba969eeb9261ada9"
            ],
            "layout": "IPY_MODEL_dbedf73117dc4e34818b51e3e534d980"
          }
        },
        "84e8de2ed794435a874c16b5602980a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bcbdaa97e4f49409f3799e2f8326582",
            "placeholder": "​",
            "style": "IPY_MODEL_1899ab5a846645739a06174574730974",
            "value": "Map: 100%"
          }
        },
        "c7bdab274bb84dc28e8ab950164bcceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef091da726694271b327cfb482935fc1",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_220d6b61416f49828bf40a79e938da2b",
            "value": 25000
          }
        },
        "6648299b768b4842ba969eeb9261ada9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cf0d307dbdc4ac2a2ce621245164770",
            "placeholder": "​",
            "style": "IPY_MODEL_b415da28767f444cbc5b404d06987fbd",
            "value": " 25000/25000 [02:22&lt;00:00, 207.77 examples/s]"
          }
        },
        "dbedf73117dc4e34818b51e3e534d980": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bcbdaa97e4f49409f3799e2f8326582": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1899ab5a846645739a06174574730974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef091da726694271b327cfb482935fc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "220d6b61416f49828bf40a79e938da2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cf0d307dbdc4ac2a2ce621245164770": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b415da28767f444cbc5b404d06987fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "265f61e2043245fab4b4b87158487a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e372fb75f9b496494333b18fbd6c646",
              "IPY_MODEL_da33034e0cf6438ea2b8e80f288ab101",
              "IPY_MODEL_ee60337c308043ff9d4316ff201ce730"
            ],
            "layout": "IPY_MODEL_0d169282a0e5419fb147e3a208a202fe"
          }
        },
        "7e372fb75f9b496494333b18fbd6c646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7db76dd18b3e49db9ab71c594da74cc9",
            "placeholder": "​",
            "style": "IPY_MODEL_cfd09e7ef6a74490b243de7c6ae8b622",
            "value": "Map: 100%"
          }
        },
        "da33034e0cf6438ea2b8e80f288ab101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2bc299ecdae4bec989815dc2e7bb6c4",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bc124e1669a4810847746adca4f2cc9",
            "value": 50000
          }
        },
        "ee60337c308043ff9d4316ff201ce730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60051234adb0444ba68a6578a4a94c9e",
            "placeholder": "​",
            "style": "IPY_MODEL_47064e16669e4714a8838f6ae549e387",
            "value": " 50000/50000 [04:13&lt;00:00, 205.13 examples/s]"
          }
        },
        "0d169282a0e5419fb147e3a208a202fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db76dd18b3e49db9ab71c594da74cc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfd09e7ef6a74490b243de7c6ae8b622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2bc299ecdae4bec989815dc2e7bb6c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bc124e1669a4810847746adca4f2cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60051234adb0444ba68a6578a4a94c9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47064e16669e4714a8838f6ae549e387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 1\n",
        "\n",
        "## Tokenization & Embeddings\n",
        "\n",
        "### What You’ll Learn:\n",
        "\n",
        "* How LLMs process text using tokenization\n",
        "* How to convert text into embeddings\n",
        "* Why embeddings are crucial for NLP tasks\n",
        "\n",
        "### Real-World Application\n",
        "* Chatbots & Virtual Assistants: Extracting sentence embeddings to match user queries with relevant responses.\n",
        "* Search & Information Retrieval: Converting documents into embeddings for similarity search (used in Google Search, Semantic Search, etc.).\n",
        "* Sentiment Analysis & Text Classification: Embeddings serve as features for classification models in finance, healthcare, and marketing."
      ],
      "metadata": {
        "id": "059Gun7ndVzt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x95Ul1ZmdIVk"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load a pre-trained tokenizer and model (BERT in this case)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Sample text for tokenization\n",
        "text = \"Large Language Models are revolutionizing NLP.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Tokenize the text\n",
        "tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "# View\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBHnOJ30evbB",
        "outputId": "65b22f85-983c-441e-a410-03efbee6ba12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2312,  2653,  4275,  2024,  4329,  6026, 17953,  2361,  1012,\n",
              "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The goal is to adjust the model's weights to minimize the error between its predictions and the actual targets.\n",
        "* This involves calculating gradients (the rate of change of the error with respect to the weights) and using them to update the weights.\n",
        "* During training, PyTorch keeps track of all operations performed on tensors that have requires_grad=True to enable gradient calculation.\n",
        "\n",
        "Inference:\n",
        "* Since we're using a pre-trained model to make predictions on new data, **the model's weights are fixed, so there's no need to calculate gradients.**\n",
        "* Calculating and storing gradients during inference is a waste of computational resources and memory."
      ],
      "metadata": {
        "id": "dtZzjRGrfmo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Generate embeddings using the model\n",
        "with torch.no_grad():  # No gradient calculation needed for inference\n",
        "    output = model(**tokens)"
      ],
      "metadata": {
        "id": "ERvmdwG0ew9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`output.last_hidden_state`: This represents the final hidden state of the model, which is a tensor containing the contextualized representations of each token in the input sequence."
      ],
      "metadata": {
        "id": "qccsI_NLgWIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Extract the embeddings (CLS token represents the sentence embedding)\n",
        "embedding = output.last_hidden_state[:, 0, :]"
      ],
      "metadata": {
        "id": "4hsDUs21eyeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shape of embedding vector\n",
        "print(\"Embedding Shape:\", embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gccQL7tpe0ie",
        "outputId": "2f6cdc40-822b-44d5-efd2-4a7530989da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Shape: torch.Size([1, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is what the text became. A tensor with numbers for the LLM to understand.\n",
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK-MegCIe1ZH",
        "outputId": "5fbcc78b-c37d-4ed8-a5d7-1aaff0300f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.4580e-01, -1.7613e-01,  9.1574e-02, -3.1258e-01, -4.3672e-01,\n",
              "         -4.6326e-01,  3.5836e-01,  6.3136e-01, -1.6741e-01, -6.0901e-01,\n",
              "          1.5760e-02,  2.0739e-01, -3.9844e-01,  1.2286e-01,  2.5884e-01,\n",
              "          2.2635e-01, -2.9151e-01,  5.7592e-01,  2.7204e-01,  7.5218e-02,\n",
              "         -3.6411e-01, -6.5309e-01,  1.4161e-01, -5.1403e-02,  6.1204e-02,\n",
              "         -1.0301e-01,  8.3905e-02, -3.2397e-01,  4.2635e-02, -2.5525e-01,\n",
              "         -5.8510e-01,  4.0276e-01, -1.2440e-01, -4.1155e-01,  4.0300e-01,\n",
              "         -9.8790e-02, -2.1416e-01, -3.4497e-01,  6.1450e-01,  4.2820e-01,\n",
              "         -6.4149e-01,  2.3772e-02,  5.7509e-02, -2.9054e-01, -2.8852e-01,\n",
              "         -2.0358e-01, -3.3098e+00, -3.1943e-01, -3.3256e-01, -3.7594e-01,\n",
              "         -4.7578e-01,  6.8495e-02, -1.1553e-01,  3.6843e-01,  2.6826e-01,\n",
              "          2.3217e-01, -1.3763e-01,  2.9508e-01,  3.0863e-02, -1.1439e-02,\n",
              "          7.3251e-02,  4.7569e-01, -3.8645e-01, -2.8943e-01, -4.0224e-01,\n",
              "          2.9392e-01, -4.6213e-01,  1.6991e-01, -5.8971e-01,  7.8490e-02,\n",
              "         -3.5409e-01, -3.9310e-01,  4.6377e-01,  3.4963e-03, -1.9337e-01,\n",
              "          3.2278e-01, -7.7574e-02,  1.9968e-01, -3.6294e-01, -2.2592e-01,\n",
              "         -1.1562e-01,  7.7869e-01,  4.7206e-01, -3.3103e-01,  7.4534e-01,\n",
              "          6.6025e-01,  7.3010e-02,  2.2219e-01,  5.1176e-01,  6.9484e-01,\n",
              "         -5.7265e-01, -5.4014e-01, -4.8113e-03,  6.2205e-01,  6.8074e-01,\n",
              "          4.4843e-01,  7.4932e-02, -1.9152e-01,  1.1133e-01,  4.0024e-01,\n",
              "          6.7736e-01, -3.6012e-01,  2.8387e-01, -3.1617e-01, -1.9144e-02,\n",
              "         -1.2925e-01, -3.1176e-01, -1.2356e-01, -1.4631e-02, -2.6526e+00,\n",
              "          4.9119e-01, -8.0358e-02, -4.6664e-01, -7.5003e-01,  2.1526e-03,\n",
              "          5.6888e-01,  3.7308e-01,  3.5440e-02, -2.3440e-02,  3.3655e-02,\n",
              "          1.4000e-01,  9.4836e-01, -1.3186e-01,  1.3969e-01, -2.9444e-01,\n",
              "          1.4526e-01,  1.8502e-01, -2.3002e-03,  5.2684e-01,  1.4610e-01,\n",
              "          4.5049e-01,  5.5242e-01, -8.0223e-02, -4.0284e-01,  9.6639e-03,\n",
              "          3.6563e-01,  4.4831e-01,  2.9514e-01, -3.8523e-01, -4.6545e-02,\n",
              "         -4.9957e-01, -2.5162e-01, -2.7927e+00,  1.5734e-01,  8.2287e-01,\n",
              "          3.8344e-01, -4.8059e-01, -6.0850e-01,  2.0890e-01, -4.6563e-02,\n",
              "          3.8066e-01,  1.3339e-01, -2.5477e-01,  3.8595e-02, -3.9065e-01,\n",
              "          4.1622e-01, -4.9422e-01, -2.9342e-01,  4.5133e-01,  5.7522e-01,\n",
              "         -4.2255e-01, -4.6471e-01, -3.0050e-01, -2.1879e-01, -3.8034e-01,\n",
              "          1.9515e-01,  4.5972e-01,  3.1535e-01,  1.9257e-01,  2.3050e-02,\n",
              "         -1.3596e-01, -2.1409e-02,  3.6076e-01,  8.2561e-02,  1.0244e-02,\n",
              "         -2.1675e-01,  2.2335e-01,  1.5189e-01,  1.7846e-01, -3.9597e-02,\n",
              "         -4.3160e-01,  5.7166e-01,  4.0424e-01,  1.3185e-01,  4.9273e-01,\n",
              "         -4.7383e-01,  3.5783e-01, -2.6817e-01, -4.9641e-01, -3.6670e-01,\n",
              "         -1.8004e-01, -2.6779e-01,  1.0165e-01, -7.5910e-02,  7.2126e-01,\n",
              "          4.2992e-01,  1.8358e-01,  1.3301e-02,  5.6957e-02,  3.0170e-01,\n",
              "         -3.2363e-01,  5.6948e-02, -3.3557e-01, -1.5295e-01,  8.6689e-02,\n",
              "          3.5105e+00,  2.3801e-02, -1.4811e-01,  1.9692e-01, -1.9230e-01,\n",
              "         -1.9509e-01, -2.1099e-01,  7.8688e-02, -1.0513e-01,  2.2584e-01,\n",
              "         -7.4215e-02,  2.1705e-01, -4.9612e-02, -5.7464e-01, -1.6556e-02,\n",
              "          1.4868e-01,  2.3676e-01, -2.1978e-01,  8.5535e-02,  5.3609e-02,\n",
              "          4.7473e-01,  7.1861e-01,  5.4882e-01,  7.8140e-01, -1.2806e+00,\n",
              "         -2.3333e-03, -2.9732e-01, -2.3647e-01,  2.0712e-01, -4.2221e-01,\n",
              "         -3.7668e-01, -1.1529e-01, -3.0876e-01, -2.8912e-01, -1.4449e-01,\n",
              "         -7.9718e-02,  6.5677e-01,  2.6297e-02,  1.4729e-01,  2.4880e-01,\n",
              "         -2.5442e-01,  1.9207e-01, -2.5409e-01,  1.7192e-01, -5.2350e-01,\n",
              "          5.7259e-01,  2.3247e-01,  1.0697e-01, -6.8419e-02,  1.4490e-01,\n",
              "          2.3780e-01,  1.9269e-01,  3.2564e-01, -3.8204e-01,  1.6893e-01,\n",
              "         -2.2858e-01, -1.4732e-02,  1.0358e-01,  1.1273e-01, -4.5965e-01,\n",
              "         -6.2142e-01,  2.2984e-01, -2.2334e-01,  4.5782e-01,  8.9510e-02,\n",
              "         -1.2691e-01, -5.9502e-01, -1.3069e-01, -3.8581e+00,  1.1228e-01,\n",
              "         -1.7430e-01,  1.4767e-01,  1.2648e-01, -3.1226e-01, -2.4641e-01,\n",
              "          1.7789e-01,  5.5386e-03, -1.8739e-01,  8.0352e-02,  4.7290e-02,\n",
              "         -2.5014e-02,  4.0335e-02, -5.9218e-01,  3.8313e-02,  1.9708e-01,\n",
              "         -4.6631e-01, -3.9466e-01, -2.2117e-01,  1.7999e-01, -5.9804e-01,\n",
              "         -3.1196e-01,  3.0859e-01,  4.6721e-02, -1.5947e-01,  3.9775e-02,\n",
              "         -5.2560e-01, -6.0236e-01,  1.7380e-02,  2.3846e-01, -6.7025e-01,\n",
              "         -2.5619e-02, -1.1760e-01,  3.2407e-01, -2.2169e+00,  3.6415e-01,\n",
              "          1.0103e-01, -3.1471e-02, -4.3502e-02, -5.9668e-02,  3.2815e-01,\n",
              "         -4.8168e-01, -1.4371e-01, -1.4818e-02,  1.6957e-01, -1.1899e-01,\n",
              "          2.5814e-02,  2.3489e-01,  1.9667e-01,  5.4146e-01,  7.4145e-01,\n",
              "          7.9662e-02,  2.8883e-01,  5.9342e-02, -4.0074e-02, -4.7021e-02,\n",
              "         -2.6239e-01, -4.4875e-01,  1.6281e-01,  1.2479e-01, -6.6528e-01,\n",
              "         -2.6622e-01, -5.8095e-01, -2.9434e-01, -7.6011e-02, -2.5663e-01,\n",
              "          1.5413e-01, -5.0196e-01, -5.4056e-01, -1.5617e-01, -1.5267e-02,\n",
              "          5.2895e-01,  6.1707e-01, -5.0685e-01,  2.3922e-01,  7.1372e-01,\n",
              "          1.0036e-01,  1.0639e-01,  3.2475e-01,  4.7295e-02, -2.3706e-01,\n",
              "          7.8796e-02,  1.3826e-01,  3.5275e-02, -2.8379e-01, -1.6275e-02,\n",
              "          1.2161e+00,  3.0899e-02,  4.0886e-01, -1.9114e-01,  5.2142e-01,\n",
              "         -1.7181e-01,  2.7966e-01,  3.4509e-01,  1.0205e+00, -5.1943e-01,\n",
              "          2.1300e-01, -5.0663e-01,  1.6683e-01, -5.2443e-01,  2.6700e-01,\n",
              "         -7.1714e-01,  6.6597e-02, -3.7289e-02, -1.3647e-01,  9.1808e-01,\n",
              "         -5.1762e-01, -1.0993e+00, -1.2927e-01, -5.0609e-01,  1.2654e-01,\n",
              "         -1.1472e-02,  1.5717e-01,  2.4976e-01, -2.4439e-01, -2.7673e-02,\n",
              "         -6.9838e-01,  1.2238e-01, -1.1869e-01, -5.5125e-01, -1.8425e-01,\n",
              "          2.9146e-01, -8.7630e-01, -2.6286e-01, -7.0947e-02,  1.2652e-01,\n",
              "          3.4961e-01, -7.0887e-03,  2.3103e-01, -2.1621e-01,  3.7399e-01,\n",
              "         -4.1712e-01, -2.6570e-02, -2.4073e-01, -3.0370e-01, -4.5034e-01,\n",
              "         -4.4816e-01,  9.8133e-02,  8.4535e-02, -4.3303e-02, -1.5912e-01,\n",
              "          3.8874e-01,  2.2482e-01,  4.0380e-02, -3.7889e-01,  3.4949e-01,\n",
              "          1.5975e-01,  2.5997e-02,  7.8795e-01,  2.0577e-01, -9.5454e-02,\n",
              "          6.0667e-01,  1.8262e-01, -1.5899e-01,  3.8248e-01,  4.5702e-01,\n",
              "          2.8972e-01,  3.4033e-02,  8.6724e-02,  3.8522e-01,  7.3283e-02,\n",
              "          2.8863e-01, -5.9009e-01, -7.6705e-01,  9.3003e-02, -3.7393e-01,\n",
              "         -2.6894e-01, -5.9855e-01, -1.6939e-01, -6.2491e-01, -1.4368e-01,\n",
              "          8.5274e-01,  7.4380e-02, -1.7920e-01,  4.7987e-01,  5.5639e-02,\n",
              "         -1.5621e-01,  1.3189e-01,  1.7666e-01,  5.4593e-01, -2.3071e-01,\n",
              "          1.6692e-01, -1.6217e-01,  5.6510e-01, -2.8948e-02, -4.4324e-01,\n",
              "         -2.4149e-01, -2.0349e-01,  4.2586e-01,  1.7107e-01, -1.5032e-01,\n",
              "         -1.2906e-01,  7.0639e-02, -1.9279e-01,  3.6609e-01,  2.4879e-01,\n",
              "         -1.6265e+00,  5.9089e-02,  4.6913e-01, -2.7437e-01,  1.1817e-01,\n",
              "         -6.4897e-02, -5.7622e-01,  5.9617e-01, -7.1542e-02, -1.7433e-01,\n",
              "         -4.6564e-01, -2.0654e-01,  2.5238e-02,  7.5361e-02,  5.2544e-01,\n",
              "         -1.9917e-01, -2.2587e-01, -2.3679e-01,  2.4004e-01,  4.4510e-02,\n",
              "         -1.1383e-01,  3.7681e-01, -2.0766e-02,  4.3580e-02,  3.6578e-01,\n",
              "          4.9609e-02, -5.3733e-01,  3.2243e-02,  4.4181e-01,  1.0694e+00,\n",
              "         -1.6214e-02, -4.0956e-01, -8.2479e-01, -9.5772e-02,  3.8152e-01,\n",
              "         -1.1708e-01,  1.5997e-01, -3.4399e-01,  7.0266e-01,  4.5002e-01,\n",
              "         -3.9848e-01,  4.7370e-01,  3.0240e-01,  2.5694e-02,  2.6038e-02,\n",
              "          2.0274e-01, -5.5065e-01,  4.5901e-01, -2.4043e-01, -3.7438e-01,\n",
              "         -2.9793e-02, -3.3582e-01, -1.9701e-01, -1.3368e-01,  4.2432e-02,\n",
              "          2.4355e-01, -2.8365e-01,  2.1218e-01, -8.2527e-01, -2.6717e-01,\n",
              "         -9.8915e-02, -4.1160e-01, -5.5291e-01,  3.8085e-02, -7.5894e-01,\n",
              "         -8.1174e-01, -2.3875e-01, -6.5167e-01, -2.2434e-01, -1.5643e-01,\n",
              "          4.0670e-01,  5.0183e-01, -3.4549e-01,  3.2724e-01, -3.2205e-01,\n",
              "          5.8213e-01, -1.9013e-01, -3.7820e-01,  3.6301e-01,  4.2486e-01,\n",
              "         -1.9689e-01, -2.4502e-01, -7.1291e-02,  6.6330e-01,  2.6719e-01,\n",
              "         -7.8175e-02, -2.4519e-01, -1.5076e-01, -3.4205e-02, -1.0746e-01,\n",
              "         -4.7401e-02, -4.0583e-01,  4.8706e-01,  4.0523e-01,  2.1426e-01,\n",
              "         -2.1939e-01,  1.0738e-01, -3.1287e-01,  3.5788e-01, -6.9499e-01,\n",
              "         -1.7030e-01,  4.7036e-01,  3.8368e-01,  3.3180e-01,  2.0687e-01,\n",
              "         -2.4921e-02,  6.4098e-01,  3.0968e-01, -2.8014e-01, -2.4939e-01,\n",
              "          4.0863e-02, -1.2405e-01, -7.5553e-02, -9.5447e-02,  1.9759e-01,\n",
              "         -6.6785e-01,  4.4251e-01, -4.8660e-01,  2.1951e+00,  1.2364e-01,\n",
              "          3.4739e-01, -4.9608e-01,  6.5550e-01,  1.6791e-01,  2.5223e-01,\n",
              "          1.6638e-01, -7.6247e-02,  2.4831e-01, -4.6471e-01,  2.4447e-01,\n",
              "          1.1968e-01,  6.3706e-01,  3.7341e-01,  4.8555e-01,  3.8025e-02,\n",
              "         -3.0084e-01, -7.7911e-01, -2.0418e-01, -3.2717e-01,  1.2014e+00,\n",
              "          1.0731e-01, -1.5164e-01,  1.9251e-01,  4.3094e-01,  1.3150e-01,\n",
              "          2.5983e-01, -6.5113e-03,  2.8405e-01, -4.5058e-01,  5.7193e-01,\n",
              "          5.2839e-01,  3.8187e-01, -1.8242e-01,  9.8936e-02,  1.5880e-02,\n",
              "         -3.7911e-02,  2.5100e-01, -5.2037e-02, -1.5758e-01, -4.2361e-01,\n",
              "          5.6899e-01,  1.1123e-01, -2.8610e-01,  9.7398e-01,  1.3787e-01,\n",
              "         -4.5038e-01,  2.7675e-01,  2.9276e-01, -2.2751e-02,  3.7252e-01,\n",
              "          4.1807e-02,  1.8051e-02, -1.7916e-01, -5.0536e-01,  1.1446e-01,\n",
              "          3.6061e-01,  2.1237e-01,  2.2313e-01,  5.2490e-01,  2.0784e-01,\n",
              "          2.2996e-02, -1.3304e-01, -1.5833e-01, -3.2443e-02, -4.0943e-01,\n",
              "          6.8783e-01, -1.6720e-01, -2.8676e-01,  2.6001e-01,  4.4500e-01,\n",
              "          2.0766e-01,  6.7856e-01,  4.8561e-01, -8.5235e-02,  5.6404e-01,\n",
              "         -1.0501e-01, -2.3849e-01, -2.5252e+00,  1.5237e-01,  3.1318e-01,\n",
              "          1.0191e+00,  5.2900e-02,  3.6416e-01,  2.0692e-01, -3.9689e-02,\n",
              "          4.4024e-01, -1.5883e-03,  8.5594e-03,  2.2420e-01,  7.7898e-01,\n",
              "          1.4708e-01,  1.9458e-02,  1.1520e-01, -6.7271e-02, -4.7791e-01,\n",
              "          2.5795e-02, -2.5344e-01,  1.8114e-01, -1.3095e-01,  2.4854e-01,\n",
              "          1.8459e-01, -3.9327e-01,  1.2125e-01,  5.7689e-01, -2.4932e-01,\n",
              "          2.2852e-01,  4.5045e-01, -1.6645e-01,  6.8766e-02, -1.7165e-03,\n",
              "          5.4247e-01,  2.5319e-01,  9.1199e-03,  3.3176e-01, -4.2260e-01,\n",
              "          1.9892e-01, -7.6606e-02, -1.7659e-01,  7.1597e-01, -7.7140e-02,\n",
              "          2.1343e-01, -9.1261e-02, -4.6934e-01,  2.7539e-01, -4.6441e-01,\n",
              "          5.1172e-01, -1.9616e-01, -7.9230e-03,  4.4318e-01,  1.6507e-01,\n",
              "          1.3407e-02,  4.7260e-01, -9.8143e-02,  4.3134e-01, -5.6042e-02,\n",
              "          1.3321e-01, -2.1923e-01, -8.1790e-02, -4.5026e-02,  9.9986e-02,\n",
              "          3.9229e-01, -3.3395e-02, -7.8257e-02,  2.8723e-01,  1.4927e-01,\n",
              "         -2.9494e-01, -3.5135e-01,  1.2229e-03,  7.0619e-02,  7.2467e-01,\n",
              "          2.7448e-01,  9.9011e-02,  4.8995e-03,  2.7443e-01,  2.5723e-02,\n",
              "          3.0341e-01,  1.9391e-01, -1.7009e-01, -1.9636e-01, -1.8316e-01,\n",
              "          1.0544e-01,  5.9650e-01, -7.7022e+00, -2.6696e-01, -5.1764e-01,\n",
              "         -7.2573e-01,  1.0893e-02, -6.6022e-01,  5.2190e-02,  1.6160e-01,\n",
              "         -1.9017e-01, -1.5907e-01,  4.7907e-02,  5.5601e-01,  7.5720e-02,\n",
              "         -4.8495e-01,  1.6084e-02,  3.4603e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 2\n",
        "\n",
        "## Transformer Architecture Basics\n",
        "\n",
        "### What You’ll Learn:\n",
        "\n",
        "* How the Attention Mechanism works in Transformers\n",
        "* How to build a Mini-Transformer Model from scratch using PyTorch.\n",
        "\n",
        "### Real-World Application\n",
        "* Machine Translation (Google Translate): Transformers power translation models like MarianMT and M2M-100.\n",
        "* Speech Recognition (Whisper, DeepSpeech): Used in voice assistants like Siri and Alexa.\n",
        "* Text Summarization (GPT, T5, BART): Automatic document summarization for legal, medical, and news texts."
      ],
      "metadata": {
        "id": "YkI1nZ6ggl05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a simple self-attention mechanism\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads  # Splitting embedding into multiple heads\n",
        "\n",
        "        # Query, Key, and Value matrices\n",
        "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, value, key, query, mask=None):\n",
        "        N = query.shape[0]  # Batch size\n",
        "        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]\n",
        "\n",
        "        # Apply linear transformations\n",
        "        values = self.values(value)\n",
        "        keys = self.keys(key)\n",
        "        queries = self.queries(query)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        energy = torch.matmul(queries, keys.transpose(-2, -1)) / (self.embed_size ** (1/2))\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))  # Apply mask for padding\n",
        "\n",
        "        attention = torch.softmax(energy, dim=-1)  # Normalize with softmax\n",
        "\n",
        "        # Multiply attention weights by values\n",
        "        out = torch.matmul(attention, values)\n",
        "        return self.fc_out(out)\n",
        "\n",
        "# Define a Transformer Encoder block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        # Feedforward network\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        x = self.norm1(attention + query)  # Residual connection + Layer Norm\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.norm2(forward + x)  # Residual connection + Layer Norm\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "4PFGm-Wke4qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Attention: The core idea is to let the model focus on relevant parts of the input.\n",
        "* Multi-head Attention: Instead of just one way to focus, the model uses multiple \"heads,\" each focusing on different aspects of the relationships between words.\n",
        "* Attention Head: Each head is a separate, parallel attention mechanism. It learns its own set of weights to calculate attention scores, allowing it to capture different patterns and relationships within the data."
      ],
      "metadata": {
        "id": "z-p_KcKmiJ08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "embed_size = 512  # Size of word embeddings\n",
        "heads = 8  # Number of attention heads\n",
        "dropout = 0.1\n",
        "forward_expansion = 4"
      ],
      "metadata": {
        "id": "TKtaSC9uhewl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Transformer block\n",
        "transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)"
      ],
      "metadata": {
        "id": "zNRHy_t2hen8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input (batch_size=2, seq_len=5, embed_size=512)\n",
        "x = torch.rand((2, 5, embed_size))"
      ],
      "metadata": {
        "id": "hspYLggxhelf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer Block:\n",
        "This refers to a fundamental building block of the Transformer architecture. It typically consists of:\n",
        "* Multi-head attention.\n",
        "* Feed-forward neural networks.\n",
        "Layer normalization and residual connections.\n",
        "* Forward Pass:\n",
        "This means you're feeding input data (x) through the Transformer block to see what output it produces. It's the process of calculating the output of the network given an input.\n",
        "* x, x, x:\n",
        "In a standard self-attention mechanism, the Transformer block takes three inputs: query, key, and value. In your code, you're using the same input x for all three. This is common in self-attention, where the model attends to different parts of the same input sequence.\n",
        "* mask=None:\n",
        "The mask parameter is used to prevent the model from attending to certain parts of the input sequence. Setting it to None means no masking is applied, so the model can attend to all positions.\n",
        "* output = transformer_block(x, x, x, mask=None):\n",
        "This line executes the forward pass. The transformer_block processes the input and produces an output tensor.\n",
        "* print(\"Transformer Output Shape:\", output.shape):\n",
        "This line prints the shape of the output tensor. This tells you the dimensions of the output, which are crucial for understanding how the data is transformed by the Transformer block."
      ],
      "metadata": {
        "id": "YjEBy63ein6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass through Transformer\n",
        "# In essence, you took some input data (x), passed it through one layer of a transformer, and then printed the shape of the result. You are seeing what the output of that layer is.\n",
        "output = transformer_block(x, x, x, mask=None)\n",
        "print(\"Transformer Output Shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjs7qzywhejP",
        "outputId": "8eeb5f7a-d782-4dc7-c847-b746b06d7cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer Output Shape: torch.Size([2, 5, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 3\n",
        "\n",
        "## Fine-Tuning LLMs (BERT for Text Classification)\n",
        "### 💡 What You’ll Learn:\n",
        "\n",
        "* How to fine-tune a pre-trained LLM (BERT) on a text classification task\n",
        "* How to train the model on a real dataset using Hugging Face’s transformers library\n",
        "* Why fine-tuning is crucial for adapting LLMs to specific domains\n",
        "\n",
        "\n",
        "### 📌 Real-World Applications\n",
        "* Customer Feedback Analysis: Companies analyze sentiment from reviews to improve products/services (e.g., Amazon, Google Reviews).\n",
        "* Social Media Monitoring: Detecting positive/negative sentiment in tweets, Facebook posts, and Reddit comments.\n",
        "* Healthcare & Finance: Analyzing patient reports, legal contracts, and financial news for risk assessment."
      ],
      "metadata": {
        "id": "kRznOT_WhaLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #! pip install datasets --quiet"
      ],
      "metadata": {
        "id": "QOC7AeoFkPIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f63ea9-b46d-4f2e-b4b5-e66c21500dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If your text is in English, the tokenizer `\"bert-base-uncased\"` is a common starting point.<br>\n",
        "* For other languages, you'll need a multilingual BERT `\"bert-base-multilingual-cased\"` or a language-specific BERT.\n",
        "* If your text is from a specialized domain (e.g., medical, legal, scientific), consider using a BERT model that has been pretrained on that domain\n",
        "* `\"bert-base-uncased\"` is a smaller model, while \"bert-large-uncased\" is larger. Larger = more computational power"
      ],
      "metadata": {
        "id": "z3NN_6e2l6cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Load a sample sentiment classification dataset (IMDB reviews)\n",
        "dataset = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "EYLrBVcahbyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    return {key: torch.tensor(val) for key, val in tokenized.items()} #Convert to tensors here.\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "6631d06f0c544c8fb66533bef14958a5",
            "934b66da556d41f28d48e86f8fa8c89b",
            "653f6256421b4656a176c0cb97ab5c7c",
            "5022bcdcd7964b50ad34856693587219",
            "376e67834033415db5f9c9407f9b369c",
            "49912788b73849a6944a04be7baa7c7f",
            "0e7a79c5e2764d2681913b43161cbc14",
            "d08cc437a4c84a8cb5c513c3523f87ea",
            "41ca3d16a7a0453ea25f4f6d78f4b717",
            "4c6430a7544c408b955b5bab990b4955",
            "8255e9e3e456427197606492f5b2f85f",
            "c3907823f5df48ba8be598fde00d85ea",
            "84e8de2ed794435a874c16b5602980a2",
            "c7bdab274bb84dc28e8ab950164bcceb",
            "6648299b768b4842ba969eeb9261ada9",
            "dbedf73117dc4e34818b51e3e534d980",
            "7bcbdaa97e4f49409f3799e2f8326582",
            "1899ab5a846645739a06174574730974",
            "ef091da726694271b327cfb482935fc1",
            "220d6b61416f49828bf40a79e938da2b",
            "8cf0d307dbdc4ac2a2ce621245164770",
            "b415da28767f444cbc5b404d06987fbd",
            "265f61e2043245fab4b4b87158487a0a",
            "7e372fb75f9b496494333b18fbd6c646",
            "da33034e0cf6438ea2b8e80f288ab101",
            "ee60337c308043ff9d4316ff201ce730",
            "0d169282a0e5419fb147e3a208a202fe",
            "7db76dd18b3e49db9ab71c594da74cc9",
            "cfd09e7ef6a74490b243de7c6ae8b622",
            "d2bc299ecdae4bec989815dc2e7bb6c4",
            "8bc124e1669a4810847746adca4f2cc9",
            "60051234adb0444ba68a6578a4a94c9e",
            "47064e16669e4714a8838f6ae549e387"
          ]
        },
        "id": "Fbnc1lbdkJLV",
        "outputId": "99c82031-bb71-4d99-dc5b-6b3ea37bd465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6631d06f0c544c8fb66533bef14958a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3907823f5df48ba8be598fde00d85ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "265f61e2043245fab4b4b87158487a0a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch dataset format\n",
        "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(20))  # Use a subset for quick training\n",
        "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10))"
      ],
      "metadata": {
        "id": "VI3wIz62hLW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset['text'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "collapsed": true,
        "id": "p5MXdCkVcOnU",
        "outputId": "45bdbcba-28ec-40cb-c751-544e8be44c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra\\'s song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model for classification (2 classes: Positive/Negative)\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
        "\n",
        "# freeze all base model parameters\n",
        "for name, param in model.base_model.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# unfreeze base model pooling layers\n",
        "for name, param in model.base_model.named_parameters():\n",
        "    if \"pooler\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "# hyperparameters\n",
        "lr = 2e-4\n",
        "batch_size = 8\n",
        "num_epochs = 10\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"bert-phishing-classifier_teacher\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    logging_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "print(\"Training Complete! ✅\")"
      ],
      "metadata": {
        "id": "5TsFGBnhVwge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply model to validation dataset\n",
        "predictions = trainer.predict(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-kcu87BYkhhY",
        "outputId": "9feeaa3a-cdae-42b4-bcbb-9e1d0a07ac4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.label_ids == test_dataset['label']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxm9jNBFkrhF",
        "outputId": "ae299c3f-3a7c-441d-808a-5cae30ce35b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 4\n",
        "## RAGs\n",
        "\n",
        "### What You’ll Learn:\n",
        "\n",
        "* What RAG is and why it’s useful for LLMs\n",
        "* How to retrieve relevant documents and generate responses using FAISS and LangChain\n",
        "* How to apply RAG to augment LLMs with real-world data\n",
        "\n",
        "### Why Use RAG?\n",
        "* Large Language Models (LLMs) are limited by their training data, which may be outdated or incomplete.\n",
        "* RAG improves responses by retrieving relevant external knowledge before generating text.\n",
        "\n",
        "### Real-World Applications\n",
        "* Legal & Financial Research: Quickly find relevant documents in large knowledge bases (e.g., LexisNexis, Bloomberg).\n",
        "* Medical Chatbots: Retrieve medical research papers before generating answers (e.g., PubMed).\n",
        "* Enterprise AI Assistants: Use RAG for real-time knowledge augmentation in corporate settings (e.g., internal wikis)."
      ],
      "metadata": {
        "id": "YWnZo2QdhcNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries (if not installed)\n",
        "# pip install faiss-cpu transformers langchain openai datasets\n",
        "\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load pre-trained embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load sample documents (e.g., Wikipedia articles)\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train\").shuffle(seed=42).select(range(500))  # Use a subset for speed\n",
        "\n",
        "# Convert documents into embeddings\n",
        "documents = [doc[\"text\"] for doc in dataset]\n",
        "document_embeddings = embedding_model.embed_documents(documents)\n",
        "\n",
        "# Create a FAISS index for fast retrieval\n",
        "dimension = len(document_embeddings[0])  # Embedding size\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(torch.tensor(document_embeddings).numpy())\n",
        "\n",
        "# Create a LangChain retriever\n",
        "vector_store = FAISS(embedding_function=embedding_model, index=index)\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "# Use GPT-4 as the LLM for generation\n",
        "llm = OpenAI(model_name=\"gpt-4\", temperature=0.7)\n",
        "\n",
        "# Build RAG pipeline\n",
        "qa_chain = RetrievalQA(llm=llm, retriever=retriever)\n",
        "\n",
        "# Query the RAG system\n",
        "query = \"What is the history of the Eiffel Tower?\"\n",
        "response = qa_chain.run(query)\n",
        "print(\"\\n🔍 Retrieved Answer:\\n\", response)\n"
      ],
      "metadata": {
        "id": "JzYaQs4bqpkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 5\n",
        "\n",
        "## Fine-Tuning LLMs with Custom Datasets\n",
        "### 💡 What You’ll Learn:\n",
        "\n",
        "* How to fine-tune GPT-style models on domain-specific data\n",
        "* How to prepare custom datasets for fine-tuning\n",
        "* How to train an LLM using Hugging Face’s transformers library\n",
        "\n",
        "### Why Fine-Tune an LLM?\n",
        "* Pre-trained models (like GPT-3.5, GPT-4, LLaMA) are generic, meaning they lack domain-specific knowledge.\n",
        "* Fine-tuning adapts LLMs for specialized fields (e.g., legal, finance, healthcare).\n",
        "\n",
        "### 📌 Real-World Applications\n",
        "* Chatbots & Virtual Assistants: Fine-tuned models provide domain-specific knowledge.\n",
        "* Legal & Finance AI: Fine-tune models on legal contracts, financial reports, or medical texts.\n",
        "* Enterprise AI Solutions: Adapt LLMs for company-specific knowledge."
      ],
      "metadata": {
        "id": "fOriYovWvt39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Code: Fine-Tuning a GPT Model (LLaMA-2)\n",
        "We’ll fine-tune Meta’s LLaMA-2 (7B model) on a custom dataset using the transformers library."
      ],
      "metadata": {
        "id": "dlTl8ngGwB3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers datasets peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "BgO-Ofv9vu53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a custom dataset (e.g., StackExchange for Q&A fine-tuning)\n",
        "dataset = load_dataset(\"stanfordnlp/stackoverflow\")\n",
        "\n",
        "# Load tokenizer for LLaMA-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_dataset = dataset.map(preprocess_data, batched=True)\n",
        "train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(10000))  # Subset for quick training\n"
      ],
      "metadata": {
        "id": "2PYSqJozwJGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Model and Fine Tune\n",
        "\n",
        "📌 How This Works\n",
        "* Dataset Preparation: We load a custom dataset (StackOverflow Q&A).\n",
        "* Tokenization: Converts text into LLM-readable tokens.\n",
        "* Fine-Tuning: Trains LLaMA-2 using low-bit precision (8-bit) for efficiency.\n",
        "* Model Saving: The trained model is saved for deployment."
      ],
      "metadata": {
        "id": "-DWUW2xQwOZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "\n",
        "# Load LLaMA-2 model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", load_in_8bit=True, device_map=\"auto\")\n",
        "\n",
        "# Training arguments\n",
        "(training_args = TrainingArguments(\n",
        "    output_dir=\"./llama-finetuned\",\n",
        "    per_device_train_batch_size=4, #4 training examples at a time on each device.\n",
        "    num_train_epochs=3, #3 epochs means the model will see all the training data three times.\n",
        "    save_steps=500, #Every 500 training steps, the model will save a checkpoint.\n",
        "    save_total_limit=2, #This limits the total number of saved checkpoints. oldest deleted\n",
        "    fp16=True, #This enables mixed-precision training using 16-bit floating-point numbers (FP16).\n",
        "    logging_dir=\"./logs\" #This specifies the directory where training logs will be saved.\n",
        ")\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "hM3aOrk1wJEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bJZTI0uiwJCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLpFdcr5wJAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 6\n",
        "\n",
        "## Optimizing LLM Inference with Quantization & LoRA\n",
        "💡 What You’ll Learn:\n",
        "\n",
        "* How to reduce memory usage and speed up inference for LLMs\n",
        "* How to apply quantization to shrink model size\n",
        "* How to fine-tune using LoRA (Low-Rank Adaptation) to save computation\n",
        "\n",
        "### 📌 Why Optimize Large Models?\n",
        "* LLMs (like LLaMA-2, GPT-4, Falcon) are huge and require expensive GPUs to run.\n",
        "* Using quantization and LoRA, we can:\n",
        "* ✅ Reduce GPU memory usage (2x–4x smaller models)\n",
        "* ✅ Speed up inference (faster responses in production)\n",
        "* ✅ Enable fine-tuning on consumer GPUs"
      ],
      "metadata": {
        "id": "lO84HnlJvvat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We’ll apply quantization using bitsandbytes to reduce the model’s size while keeping performance high.\n",
        "pip install transformers accelerate bitsandbytes peft"
      ],
      "metadata": {
        "id": "DGB4sihbvwRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Step 2: Load & Quantize LLaMA-2\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Set up quantization config\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Load in 4-bit mode\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"  # Use NF4 quantization\n",
        ")\n",
        "\n",
        "# Load the LLaMA-2 model in 4-bit mode\n",
        "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print(\"✅ Model successfully loaded in 4-bit mode!\")\n"
      ],
      "metadata": {
        "id": "-ztLyUu0yArZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 How Quantization Helps\n",
        "🔹 Full Model (7B parameters) → 32GB VRAM needed\n",
        "🔹 4-bit Quantized Model → Uses only ~8GB VRAM ✅\n",
        "🔹 Same accuracy, faster inference"
      ],
      "metadata": {
        "id": "aR95io-JyKWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Applying LoRA for Efficient Fine-Tuning\n",
        "Instead of training all model parameters, LoRA (Low-Rank Adaptation) freezes most weights and trains only small adapters, saving 80%+ computation."
      ],
      "metadata": {
        "id": "uUx4Wz0wyN3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,  # Rank (smaller = faster)\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05\n",
        ")\n",
        "\n",
        "# Apply LoRA adapters to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"✅ LoRA adapters applied successfully!\")\n"
      ],
      "metadata": {
        "id": "iMvO-Fu8yApQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 How LoRA Helps\n",
        "✅ Trains only 1%–10% of parameters instead of full fine-tuning\n",
        "✅ Memory-efficient → fine-tune large models on a single GPU\n",
        "✅ Easy merging → use LoRA-trained adapters on different tasks\n",
        "\n",
        "📌 Real-World Applications\n",
        "\n",
        "Deploying LLMs on Edge Devices:\n",
        "\n",
        "* Run 4-bit models on consumer GPUs or even mobile devices.\n",
        "* Speeding Up Chatbots: Reduce latency in real-time applications like AI customer support.\n",
        "* Enterprise AI Scaling: Fine-tune huge models on low-cost hardware using LoRA adapters."
      ],
      "metadata": {
        "id": "h8l9gxpZySr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lesson 7\n",
        "\n",
        "## Deploying the Optimized Multi-GPU Trained Model (FastAPI & Docker) 🚀\n",
        "Now that we’ve optimized and trained our LLM using DeepSpeed/FSDP, let's deploy it using FastAPI for real-world inference.\n",
        "\n",
        "📌 Steps to Deploy the Model\n",
        "\n",
        "1️⃣ Load the trained model & tokenizer<br>\n",
        "2️⃣ Create an API endpoint using FastAPI<br>\n",
        "3️⃣ Dockerize the API for scalable deployment<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "_4BwxYicJovp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Step 1: Install Required Dependencies"
      ],
      "metadata": {
        "id": "5jxH-QICJ2fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi uvicorn torch transformers deepspeed\n"
      ],
      "metadata": {
        "id": "WU34P4E1yAnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Step 2: Create a FastAPI Inference Server\n",
        "This script will serve the LLM and generate text from API requests.\n",
        "\n",
        "📜 app.py (FastAPI Server)"
      ],
      "metadata": {
        "id": "XEak71sNJ49g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Load model and tokenizer (optimized for multi-GPU)\n",
        "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).half().to(\"cuda\")\n",
        "\n",
        "# Define request format\n",
        "class RequestData(BaseModel):\n",
        "    prompt: str\n",
        "    max_length: int = 100\n",
        "\n",
        "@app.post(\"/generate/\")\n",
        "async def generate_text(data: RequestData):\n",
        "    inputs = tokenizer(data.prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    output = model.generate(**inputs, max_length=data.max_length)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return {\"generated_text\": response}\n",
        "\n",
        "# Run with: uvicorn app:app --host 0.0.0.0 --port 8000\n"
      ],
      "metadata": {
        "id": "tqy5KcvtyAkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Step 3: Create a Dockerfile for Deployment\n",
        "To package and run the API anywhere, let’s build a Docker container.\n",
        "\n",
        "📜 Dockerfile"
      ],
      "metadata": {
        "id": "YByZYwqNJ-Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use official PyTorch image with CUDA support\n",
        "FROM pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "# Install dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy app files\n",
        "COPY app.py .\n",
        "\n",
        "# Expose API port\n",
        "EXPOSE 8000\n",
        "\n",
        "# Run FastAPI server\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
      ],
      "metadata": {
        "id": "JvlUnMzJJ-Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Step 4: Build & Run the Docker Container"
      ],
      "metadata": {
        "id": "8j1lBfwcKI_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a requirements.txt file\n",
        "echo \"fastapi\\nuvicorn\\ntorch\\ntransformers\\ndeepseed\" > requirements.txt\n",
        "\n",
        "# Build Docker image\n",
        "docker build -t llama-api .\n",
        "\n",
        "# Run the container\n",
        "docker run -p 8000:8000 --gpus all llama-api\n"
      ],
      "metadata": {
        "id": "V-E8P3ESKLun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Step 5: Test the API\n",
        "Once running, send a POST request to generate text.\n",
        "\n",
        "Using curl"
      ],
      "metadata": {
        "id": "gTRpSEXoKMan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "curl -X 'POST' 'http://localhost:8000/generate/' \\\n",
        "     -H 'Content-Type: application/json' \\\n",
        "     -d '{\"prompt\": \"Once upon a time,\", \"max_length\": 50}'\n"
      ],
      "metadata": {
        "id": "GzRAncumKQlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or using Python"
      ],
      "metadata": {
        "id": "qvF7Ba9LKSj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://localhost:8000/generate/\"\n",
        "data = {\"prompt\": \"The future of AI is\", \"max_length\": 50}\n",
        "response = requests.post(url, json=data)\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "F5NS87nfKTcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Real-World Deployment\n",
        "* Deploy on Cloud (AWS, Azure, GCP): Use GPU-enabled VM (A100, V100)\n",
        "* Scale with Kubernetes: Run multiple containers for load balancing\n",
        "* Serve via API Gateway: Expose API securely for enterprise use"
      ],
      "metadata": {
        "id": "zZammSBDKa36"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHPs1IGTKc5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 8\n",
        "\n",
        "## 📌 Python Code: Fine-Tuning LLM on Healthcare Data\n",
        "\n",
        "We'll take a pre-trained LLM (like GPT-2 or LLaMA) and fine-tune it on a healthcare dataset. Let's assume you have a corpus of medical texts or patient dialogues.\n"
      ],
      "metadata": {
        "id": "uTo15BMwKxLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "B1OlYg7VK42o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets accelerate\n"
      ],
      "metadata": {
        "id": "OTFChf5HK3O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2:\n",
        "Load and Preprocess Healthcare Data\n",
        "\n",
        "For fine-tuning, we need a dataset with medical text. We can use datasets like huggingface or any domain-specific dataset."
      ],
      "metadata": {
        "id": "IPjyzrScK9r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load a healthcare dataset (e.g., medical dialogues)\n",
        "dataset = load_dataset(\"med_dialogue\")  # Example of medical dialogue dataset\n",
        "\n",
        "# Preprocess dataset: Tokenize the data\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "# Apply the preprocessing function to the dataset\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "gb8T-pysK4Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Fine-Tune the Model on Healthcare Data"
      ],
      "metadata": {
        "id": "Ah5QhSVRLFSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Training arguments for fine-tuning\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./healthcare_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# Fine-tuning using the Trainer API\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"test\"],\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "u-ti6-zwK4FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Save and Evaluate the Fine-Tuned Model\n",
        "\n",
        "After fine-tuning, save the model and tokenizer for later use."
      ],
      "metadata": {
        "id": "wfLad2YJLIaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"./fine_tuned_healthcare_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_healthcare_model\")\n",
        "\n",
        "# Evaluate the model performance on a validation set\n",
        "eval_results = trainer.evaluate(encoded_dataset[\"test\"])\n",
        "print(f\"Evaluation Results: {eval_results}\")\n"
      ],
      "metadata": {
        "id": "QWotwqedK4DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Fine-Tuning for Finance and Legal\n",
        "You can apply a similar approach to Finance and Legal AI using industry-specific datasets:\n",
        "\n",
        "* Finance: Datasets like Financial News, Stock Market Data, and Quarterly Earnings Reports.\n",
        "* Legal: Use datasets like Legal Cases, Contracts, or Court Transcripts.\n",
        "\n",
        "Just replace the healthcare dataset with finance or legal text and adjust the tokenization and preprocessing steps accordingly."
      ],
      "metadata": {
        "id": "8k6YeLmNLQBv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sdCX3g9vLaS7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}